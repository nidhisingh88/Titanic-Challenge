

\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[all]{hypcap}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}

\date{\today}

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(randomForest)
library(ROCR)
library(caret)
library(kernlab)
library(stargazer)
library(vcd)
library(ggplot2)
library(rpart)
library(e1071)
set.seed(45)

opts_chunk$set(fig.path='/home/nidhi/Courses/TUDelft-Data Analytics/Kaggle:Titanic Challenge/Final Report/figure/DA', fig.align='center',dev='pdf', dev.args=list(family='serif'), fig.pos='!ht',
concordance=TRUE,fig.width=3.5,fig.height=3.5)
options(width=60)
@


<<mychunk, cache=TRUE, eval=TRUE,include=FALSE>>=
load(file='/home/nidhi/Courses/TUDelft-Data Analytics/Kaggle:Titanic Challenge/mydata.RData')
@

% Title Page
\begin{titlepage}
\begin{center}
% Title

\textsc{\Large SPM4450: Fundamentals of Data Analytics - Final Assignment Report }\\[6cm]

{ \bfseries \Large Performance of various Data Analytics Techniques on Kaggle's Problem Set `Titanic: Machine Learning from Disaster' \\[6cm] }

% Author and supervisor
\begin{minipage}{0.6\textwidth}
\emph{Authors:}\\
\begin{flushleft} \large
Nidhi \textsc{Singh}\\
4242246 \\
n.singh-2@student.tudelft.nl\\
MSc. Computer Science\\
\end{flushleft}

\begin{flushright} \large
Krishna Chaitanya \textsc{Akundi}\\
4239008 \\
k.c.akundi@student.tudelft.nl\\
MSc. Computer Science\\
\end{flushright}

\end{minipage}

\end{center}
\end{titlepage}

\listoffigures

\chapter{Titanic Data Set}
\section{Problem Description}
For our final assignment, we have taken up a challenge from Kaggle `Predict survival on the Titanic'. The dataset includes details of people who travelled on RMS Titanic which sank in 1912 killing 1502 out of 2224 passengers.
The aim of the Kaggle challenge is to complete the analysis of what sorts of people were likely to survive. In order to do so, we will apply different predictive models to the dataset and will finally evaluate their performance against each other. Kaggle also supports Leaderboards which evaluate the submitted results, but since this evaluation is based on only 50\% of the test data, it makes sense to do performance evaluation of all the models.

\ Since we are given both training and test data set, this problem's predictive models will fall under the umbrella of Supervised Learning Algorithms. Also we have to decide whether a passenger survived or not, this makes it a classic Classification problem.

\section{Data Exploration}
Before diving deep into prediction making on test data, we will explore the dataset. We are given two sets of data, training (data containing attributes and known outcomes [survived or perished] for a subset of the passengers) and test (data containing attributes without outcomes for a subset of passengers).The given training data set has 891 observations of following 12 variables:
\begin{itemize}
  \item PassengerId - Unique generated Id for each passenger
  \item Survived - Survival(0 = No; 1 = Yes)
  \item Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)
  \item Name - Name of the person
  \item sex - Sex 
  \item Age - Age
  \item Sibsp - Number of Siblings/Spouses Aboard
  \item Parch - Number of Parents/Children Aboard
  \item Ticket - Ticket Number
  \item Fare - Passenger Fare
  \item Cabin - Cabin in the ship
  \item Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)
  
\end{itemize}

<<echo=FALSE>>=

train_csv <- read.csv('/home/nidhi/Courses//TUDelft-Data Analytics/Kaggle:Titanic problem/train.csv')
@
Let us start by looking at the type of these variables
<<eval=TRUE,tidy=TRUE,echo=FALSE>>=
str(train_csv)
@
Here Factor refers to categorical data, since all the names are unique, we have 891 levels equal to number of observations.
<<>>=
prop.table(table(train_csv$Survived))
@
This shows that 61.6\% of the passengers perished and only 38.3\% survived.
Running the same code for Sex, we find 35.2\% females and 64.7\% in the training data set.

<<>>=
summary(train_csv$Age)
@
Summary results on Age shows that this variable is missing for 177 passengers and the minimum age is 0.42 or 5 months and maximum is 80, while 90\% of the passengers were below 50.
<<>>=
prop.table(table(train_csv$Pclass))
@
More than 55\% passengers were travelling in third class. It will be worthwhile to see the age and sex of people in each class. 

<<include=FALSE,label=ClassBySex,echo=FALSE>>=
mosaicplot(train_csv$Pclass ~ train_csv$Sex, 
           main='Passenger Class by Gender',
           shade=FALSE, 
           color=TRUE, xlab="Pclass", ylab="Sex of passenger")
@

<<include=FALSE,label=ClassByAge,echo=FALSE>>=
bpa <- ggplot(train_csv, aes(factor(Pclass), Age))
bpa + geom_boxplot()
@
<<include=FALSE,label=ClassByFare,echo=FALSE>>=
bpf <- ggplot(train_csv, aes(factor(Pclass), Fare))
bpf + geom_boxplot()+ylim(c(0,300))
@
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassBySex}
    \caption{Passenger Class by Gender.} \label{ClassBySex}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassByAge}
    \caption{Passenger Class by Age.} \label{ClassByAge}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassByFare}
    \caption{Passenger Class by Fare.} \label{ClassByFare}
  \end{subfigure}
  \caption{Passenger class by Sex, Age and Fare}\label{Class}
\end{figure}
We see in Figure\ref{Class} that third class has mostly males, since third class cabins were at the bottom of the ship this might be one of the reasons that most of the males could not survive. Also passengers in third class were younger with median below 25.With just one outlier above \$500 for first class ticket fare, fare is below \$100.

<<include=FALSE,label=Sibsp,echo=FALSE>>=
qplot(train_csv$SibSp,main='Travelling with Siblings/Spouse',xlab='No. of Siblings/Spouse')
@
<<include=FALSE,label=Parch,echo=FALSE>>=
qplot(train_csv$Parch,main='Travelling with Parents/Children',xlab='No. of Parents/Children')
@
<<include=FALSE,label=Embark,echo=FALSE>>=
qplot(train_csv$Embarked,main='Travelers Place of Embarkment',xlab='Place of Embarkment')
@

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Sibsp}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Parch}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Embark}
  \end{subfigure}
  \caption{Frequency of Sibsp, Parch and Embarkment}\label{Var}
\end{figure}
We now look at other variables to see if they can have some influence on predictions. From Figure \ref{Var} we can see that  most passengers travelled alone and started their journey from Southampton.

\ Other varibales `Ticket' and `Cabin' do not tell much as they have unique values, and are un-related to other variables.
\subsection{Survived variable with other variables}
Till now we looked at the variables and their values and frequencies and tried to get an initial understanding of the data.
Since we have to predict the `Survived' variable for the test set, in this section we will look at the relation between `Survived' variable and other variables.

As we can see from Figure \ref{Surv} age and fare doesnt seem to give much information about the survived variable, moreover most of the passengers were from Southampton so Place of Embarkment doesn't seem to play much role too.

But from Figure \ref{SurvCSE} we can find some interesting facts, people in 1st class outnumbered the people from 3rd class in survival rate.So there was a clear preference for elite poeple. From the second plot in Figure\ref{SurvCSE} we can see another preference was for females, we would like to believe that there was preference for children but this is not yet evident from our data. The last plot in Figure \ref{SurvCSE} shows that surely there was a clear bias for females in 1st and 2nd class compared to males. This is a clear indicator that `Sex' variable is hihgly important for our analysis with maybe `Pclass' coming next.
<<include=FALSE,label=SurvAge,echo=FALSE>>=
bsa <- ggplot(train_csv, aes(factor(Survived), Age))
bsa + geom_boxplot() + geom_jitter()
@
<<include=FALSE,label=SurvFare,echo=FALSE>>=
bsf <- ggplot(train_csv, aes(factor(Survived), Fare))
bsf + geom_boxplot()+ylim(c(0,300))
@
<<include=FALSE,label=SurvEmb,echo=FALSE>>=
doubledecker(Survived ~ Embarked, data=train_csv)
@
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvAge}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvFare}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvEmb}
  \end{subfigure}
  \caption{Passengers Survived by Age, Fare and Place of Embarkment}\label{Surv}
\end{figure}
<<include=FALSE,label=SurvClass,echo=FALSE>>=
doubledecker(Survived ~ Pclass, data=train_csv)
@

<<include=FALSE,label=SurvSex,echo=FALSE>>=
doubledecker(Survived ~ Sex, data=train_csv)
@
<<include=FALSE,label=SurvSC,echo=FALSE>>=
doubledecker(Survived ~ Sex + Pclass, data=train_csv)
@


\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvClass}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvSex}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvSC}
  \end{subfigure}
  \caption{Passengers survived by Class and Gender}\label{SurvCSE}
\end{figure}

\section{Feature Engineering}
There are few variables which have `NA' and missing values, before we run our prediction models we need to get rid of these. There are 177 records in the dataset where age value is NA. For these records, we assume the age to be the average age of the group.
\subsection{Title}
The title a passenger holds, such as 'Mister', 'Miss' and others, might be a useful bit of information for predicting the fate of the passenger. To get the title from the name field, part of the name string is extracted. Also, groups of similar titles are agglomerated into a single title. 

<<include=FALSE,label=FeatureTitle,echo=FALSE>>=
combi$Name <- as.character(combi$Name)
combi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
combi$Title <- sub(' ', '', combi$Title)
# Combine small title groups
combi$Title[combi$Title %in% c('Mme', 'Ms','Mlle')] <- 'Mlle'
combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
combi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
# Convert to a factor
combi$Title <- factor(combi$Title)
@
\subsection{FamilySize}
Based on the assumption that larger families might have had trouble finding each other and getting to the lifeboats, we created a feature FamilySize which combines the SibSp and Parch features already available.
<<include=FALSE,label=FeatureFamilySize,echo=FALSE>>=
combi$FamilySize <- combi$SibSp + combi$Parch + 1
@
\subsection{FamilyID}
Continuing on the basis of the same assumption, we felt it would be useful to have a feature that combines family size and surname, so we can observe how large families are affected. 
<<include=FALSE,label=FeatureFamilyID,echo=FALSE>>=
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep="")
combi$FamilyID[combi$FamilySize <= 2] <- 'Small'
@

\subsection{Training Data}
The training dataset containing 891 records was further partitioned with 80\% of it taken as the training set, and the remaining 20\% as the test set, to evaluate the performance of the models. So there were 713 records in the training partition and 178 records in the test partition.

\chapter{Prediction Models}
Since the `Predict survival on the Titanic' challenge is a classification problem, we will start with Linear Classifiers and then go further with methods like Decision trees and Ensembles of classifiers. In the following sections we will explore each model in detail and will also report its evaluation on Kaggle.

\section{Logistic Regression}
Logistic Regression is a classical Classification algorithm. R's \emph{glm} function is used to fit generalized linear models, With \emph{family} variable set to "binomial", glm( ) produces a logistic regression.The output of a linear regression can be transformed to a logit functions as follows:

$$
logit{p} = \log{o} = \log{p/1-p} = \beta_{0} + \beta_{1}x_{1} +\beta_{2}x_{2} + ...+\beta_{k}x_{k}
$$

Exponentiating this we get,
$$ \exp^{logit{p}} = \exp^{\beta_{0}} \exp^{\beta_{1}x_{1}} \exp^{\beta_{2}x_{2}}... \exp^{\beta_{k}x_{k}} $$
rewriting we get,
$$ o = p/1-p = \exp^{\beta_{0}} \exp^{\beta_{1}x_{1}} \exp^{\beta_{2}x_{2}}... \exp^{\beta_{k}x_{k}} $$
Here \emph{o} represents the \emph{odds}. From this we can say that if we know that a certain fact is true of a data point, then that will produce a constant change in the odds of the outcome.

We will run our first regression model with basic features provided within the dataset and can look at the results by calling summary on this model.Summary gives the value of \emph{estimated coefficients alongwith their standard errors and p-value} of each input variable. 

<<eval=TRUE>>=
logit.m1 <- glm(Survived ~ Pclass + Sex + Age + SibSp + 
                  Parch + Fare + Embarked, 
                data = train.batch, 
                family="binomial")
summary(logit.m1)
@

Logistic Regression uses the deviance test to estimate the goodness of the model. Its approach is based on estimating two models, it assumes that one excludes the hypothesized effects to be null, while to be included in 
the other. For each model, a deviance statistic, equal to -2 ln L for that model, is computed, which in this case is
\emph{951.76(Null model) and 634.98(estimated model)}. The deviance can be regarded as a measure of lack of fit between 
model and data. In general, the larger the deviance, the poorer the fit to the data. In our first iteration with basic
features we can see reduction of \emph{316.78} in deviance. The difference between the deviances has a large-sample chi-square distribution with degrees of freedom equal to the difference in the number of parameters estimated. Thus the difference in deviances can be tested against the chi-square distribution for significance, as done below.

<<eval=TRUE>>=
pchisq(951.76-634.98, 8)
@
Also, we can see from the summary that only \emph{Pclass, Sex, Age and SibSp(moderately)} has effect on the model with significance less than .05 level. To see how these variables affect, we can also do \emph{ANOVA} test, which tries adding the factors in the given order.
<<eval=TRUE>>=
anova(logit.m1, test="Chisq")
@
From the ANOVA results too, we can see that the deviance is highly reduced by \emph{Pclass, Sex} and weakly reduced by \emph{Age and SibSp} and with more or less no affect on deviance by other variables(Parch, Fare and Embarked).
To improve our model further we can make use of \emph{engineered variables} from previous chapter, namely \emph{Title,FamilySize and FamilyID2}

<<eval=FALSE>>=
logit.m2 <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + 
                  Embarked + Title +
                  FamilyID2,
                data = train.batch, 
                family="binomial")
summary(logit.m2)
pchisq(406.25, 36)
anova(logit.m2, test="Chisq")
@

The new model results in \emph{545.51} deviance, reducing \emph{406.25} points over null deviance of 951.76. The ANOVA test results show that the engineered variable \emph{Title and FamilyID2} have a significant effect on deviance. Thus, we will be including it in our further modelling.

We can now see how the new model performs on the test data.
<<eval=TRUE,echo=FALSE>>=
logit.predict <- predict(logit.m2, test.batch,type="response")
logit.predict.val <- ifelse(logit.predict>0.5,1,0)
@
<<eval=TRUE>>=
confusionMatrix(logit.predict.val, test.batch$Survived)
@
This model does well based on accuracy

\section{Naive Bayes}
Naive Bayes (NB) and Logistic Regression are both linear classifiers but they estimate parameters differently. Logistic Regression estimates the best model fit by minimizing the error or deviances, whereas Naive Bayes estimates `prior' probability from the training data and uses Bayes rule to predict new instances.

Since our training set has fewer instances, logistic regression might overfit, thus it would be interesting to see the performance of Naive Bayes. Also with Naive Bayes an assumption is made that our inputs are independent, thus we should not have collinear inputs in our model.
<<eval=TRUE>>=
nb.classifier <- naiveBayes(as.factor(Survived) ~ Pclass + Sex + 
                   Age + SibSp + Parch + Embarked + 
                   Title + FamilyID2, 
                  data = train.batch)
nb.predict <- predict(nb.classifier, test.batch)
@
To see the accuracy of the NB model on test data set, we can look at confusion matrix statistics
<<eval=TRUE,echo=TRUE>>=
confusionMatrix(nb.predict,test.batch$Survived)
@

The NB model performs with 82.6\% accuracy on test batch but performs poorly and same as Logistic Regression on Kaggle test batch with 77.03\% accuracy.

\section{Decision Trees}
Decision tree algorithms work by repeatedly splitting the dataset into subsets based on a particular attribute value. This process is recursively carried out until further splitting does not add any value to the predictions. This is a greedy algorithm, which means that decisions with the highest immediate value are given preference.

We applied recursive partitioning on the Titanic dataset using the Decision Tree algorithm from R's \emph{rpart} package . For this, the dependent variables used were Pclass, Sex, Age, Fare, Embarked, Title and FamilyID2. 

The algorithm was run multiple times with different control parameters, and then analyzed the results through a confusion matrix. We used the same set of features over these runs - Pclass, Sex, Age, SibSp, Parch, Embarked, Title and FamilyID2.
\subsection{}
We first ran the rpart algorithm using a minsplit value of 2 and complexity parameter of 0.01.
<<eval=T,echo=T,tidy=TRUE>>=
dt.1 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + 
                  Embarked + Title +
                  FamilyID2,
              data = train.batch, method = "class", 
              control = rpart.control(minsplit = 2, 
                                      cp = 0.01))
dt.1.pred <- predict(dt.1, test.batch, type='class')
confusionMatrix(dt.1.pred,test.batch$Survived)
@

Here, the first parameter indicates the formula for the dependent variable being estimated with respect to the independent variables, the second indicates the training data set, the third indicates the method, which is set to 'class' as the variable 'Survived' has only two levels. The last argument specifies various parameters for the working of the recursive partitioning algorithm. In our case, we specify 'minsplit', which indicates the minimum number of observations required at a node to make a split. \emph{cp} is the complexity parameter. Any split that does not reduce complexity by a factor of \emph{cp} is not made. This resulted in a model which predicted the correct outcome 76.55\% of the time.
\begin{figure}[h]
  \includegraphics[width=\textwidth]{dt1}
  \caption{Decision tree with minsplit=2 and complex parameter=.01} \label{dt1}
\end{figure}

\subsection{}
In the second run, we added control parameters which specified \emph{minsplit} as 2 and \emph{cp} as 0. This allowed unbounded growth for the tree.
<<eval=T,echo=T,tidy=TRUE>>=
dt.2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + 
                  Embarked + Title +
                  FamilyID2, data=train.batch,
             method="class",control=rpart.control(minsplit=2,cp=0.0))
dt.2.pred <- predict(dt.2, test.batch, type='class')
confusionMatrix(dt.2.pred,test.batch$Survived)
@
We ended up with a complex structure with large a number of branches. However, there were no significant gains in accuracy, sensitivity or precision. This model predicted outcomes for unseen data correctly in 71.29\% of cases. 

\begin{figure}[h]
  \includegraphics[width=\textwidth]{dt2}
  \caption{Decision tree with minsplit=2 and complex parameter as 0} \label{dt2}
\end{figure}
\subsection{}
In the last run, we specified \emph{minsplit} as 16 and \emph{cp} as 0.001.
<<eval=T,echo=T,tidy=TRUE>>=
dt.3 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + 
                  Embarked + Title +
                  FamilyID2, data=train.batch,
             method="class",control=rpart.control(minsplit=16,cp=0.001))
dt.3.pred <- predict(dt.3, test.batch, type='class')
confusionMatrix(dt.3.pred,test.batch$Survived)
@
The resulting model was better with respect to most metrics when it came to the training data, and did reasonably well with the test data, correctly predicting 77.09\% of the cases.
\begin{figure}[h]
  \includegraphics[width=\textwidth]{dt3}
  \caption{Decision tree with minsplit=16 and complex parameter=.001} \label{dt3}
\end{figure}
\section{Support Vector Machines}

Support Vector Machine (SVM) algorithms perform classification by representing the given data as points in space, with a clear plane of separation between the different classes to which the points belong. SVMs use the 'kernel trick' to work with high-dimensional data without ever having to map the points in such spaces. 
In R, the \texttt{e1071} and \texttt{kernlab} packages offer methods for creating SVM models. We used the \texttt{kernlab} package to generate our SVM models.

\subsection{}
Initially, the algorithm was run using all the features that were already present in the given dataset. The model performs well on the training dataset, with an accuracy of 84.27\%. However, it did not do so well on the test set, being able to predict the correct outcome only around 62\% of the time. More statistics from the tests are shown below.
<<eval=T,echo=T,tidy=TRUE>>=
svm.model.1 <- ksvm(Survived ~ Pclass + Sex + Age + SibSp + Parch + 
               Embarked + Title +
               FamilyID2, 
                    data = train.batch, kernel="rbfdot",
                    type="C-svc")
svm.pred.1 <- predict(svm.model.1, type="response", test.batch)
confusionMatrix(svm.pred.1, test.batch$Survived)
@
Here, the type 'C-svc' indicates that the algorithm must run in the classification mode.
\subsection{}
The algorithm was then run using a subset of the available features, containing Pclass, Sex and Embarked. 
<<eval=T,echo=T,tidy=TRUE>>=
svm.model.2 <- ksvm(Survived ~ Sex + Pclass + Embarked, 
                    data = train.batch, kernel="rbfdot",
                    type="C-svc")
svm.pred.2 <- predict(svm.model.2, type="response", test.batch)
confusionMatrix(svm.pred.2, test.batch$Survived)
@
This model proved to be slightly better performing with the training data, with an accuracy of 84.83\%, while exhibiting higher sensitivity and lower specificity. The model performed significantly better on unseen data, correctly predicting outcomes 77.99\% of the time.

\subsection{}
In the next run, the engineered features Title and FamilyID2 were used along with Pclass and Sex to create the model.
<<eval=T,echo=T,tidy=TRUE>>=
svm.model.3 <- ksvm(Survived ~ Sex + Pclass + Embarked + 
                      Title + FamilyID2, 
                    data = train.batch, kernel="rbfdot",
                    type="C-svc",
                    prob.model=TRUE)
svm.pred.3 <- predict(svm.model.3, type="response", test.batch)
confusionMatrix(svm.pred.3, test.batch$Survived)
@
This model showed slightly better performance compared to the previous one, both with the training data and the test set from Kaggle.
\subsection{}
In the final run, we constructed the SVM model using the feature set used for the other models. The performance of this model on the training set was comparable to that of the other models. On the unseen test data, the model performed well also, predicting the right outcome 78.46\% of the time.  
<<eval=T,echo=T,tidy=TRUE>>=
svm.model.4 <- ksvm(Survived ~ Pclass + Sex + Age + SibSp + Parch + 
               Embarked + Title +
               FamilyID2,
                    data = train.batch, kernel="rbfdot",
                    type="C-svc")
svm.pred.4 <- predict(svm.model.4, type="response", test.batch)
confusionMatrix(svm.pred.4, test.batch$Survived)
@

\section{Random Forests}
Random Forests are an extension of tree methods. The inherent variability of the tree model, and susceptibility to data is a problem, in random forest this is considered a strength. The intuition is that by averaging across the high variance and low bias trees we will end up with low variance low bias estimated model. We will be using R's \texttt{randomForest} package which is based on Breiman and Cutler's original Fortran code.

Before modelling we need to decide on few input variables needed for the algorithm, the number of trees to grow, \texttt{ntree}, its value should be such that each input row should get predicted a few times. Also number of variables to be sampled as candidates at each split, \texttt{mtry}, should be decided. To find the optimal value of \texttt{mtry}, we can use \texttt{tuneRF} function of \texttt{randomForest} package.
<<eval=FALSE>>=
bestmtry <- tuneRF(data.frame(train.batch$Pclass,train.batch$Sex,
                              train.batch$Age,train.batch$SibSp,
                              train.batch$Parch,train.batch$Embarked,
                              train.batch$Title,train.batch$FamilyID2),
                   train.batch$Survived, ntreeTry=100, 
                   stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE, 
                   dobest=FALSE)
@
Here \texttt{ntreeTry} is the number of trees used at the tuning step, \texttt{stepFactor} value by which mtry is inflated at every iteration. This function gives the value of \texttt{mtry} as \emph{3}. 
We now perform random forest algorithm, first with \texttt{ntree} set to 1000.
<<>>=
rf.m1 <-randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + 
                       Parch + Fare+ FamilySize +
                       Embarked + Title + FamilyID2,
                     data=train.batch, mtry=3, ntree=2000, 
                     keep.forest=TRUE, importance=TRUE)
rf.predict.1 <- predict(rf.m1, test.batch, type = "response")
confusionMatrix(rf.predict.1, test.batch$Survived)
@
and then again with \texttt{ntree} set to 2000
<<>>=
rf.m2 <-randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + 
                       Parch + Fare+ FamilySize +
                       Embarked + Title + FamilyID2,
                     data=train.batch, mtry=3, ntree=2000, 
                     keep.forest=TRUE, importance=TRUE)
rf.predict.2 <- predict(rf.m2, test.batch, type = "response")
confusionMatrix(rf.predict.2, test.batch$Survived)
@
With \texttt{ntree} set to 5000
<<>>=
rf.m3 <-randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + 
                       Parch + Fare+ FamilySize +
                       Embarked + Title + FamilyID2,
                     data=train.batch, mtry=3, ntree=5000, 
                     keep.forest=TRUE, importance=TRUE)
rf.predict.3 <- predict(rf.m3, test.batch, type = "response")
confusionMatrix(rf.predict.3, test.batch$Survived)
@
From the statistics and confusion matrix, we can see that the accuracy of the model increases by approximately 1\% when number of trees variable is increased from 1000 to 2000 but does not increase further if we set \texttt{ntree} to 5000, indicating the model has acheived its optimum(low bias and low variance) when there are 2000 trees in forest.

\chapter{Model Evaluation}
For evaluation of models made in previous chapter we will use R's package \texttt{ROCR}, its major advantage is, it eases the task of comparison by visualizing classifier performance.
An ROC curve is a way of determining the performance of classifiers. It uses statistics such as True and False Positive Rates to evaluate the performance. The statistics are calculated using a confusion matrix. 
A confusion matrix is a tabulation of expected and observed values, along with the associated statistics. It is of the form shown below.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Reference     & ~  \\ \hline
    Predicted  & Event         & No Event  \\ \hline
    Event      & A             & B \\
    No Event   & C             & D \\ \hline
    
    \end{tabular}
    \caption{Confusion Matrix}
  \end{center}
\end{table}  

Various statistics can be calculated from the confusion matrix.
\begin{itemize}
  \item{Accuracy: (A+D)/(A+B+C+D)}
  \item{95\% Confidence Interval: A measure of reliability, calculated using an exact binomial test}
  \item{No Information Rate: The largest class percentage in the data}
  \item{P-value: The probability of obtaining a test statistic result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true}
  \item{True Positive Rate: D/(A+B+C+D)}
  \item{False Positive Rate: C/(A+B+C+D) }
  \item{Sensitivity: A/(A+C)}
  \item{Specificity: D/(B+D)}
\end{itemize}

A basic ROC curve is as shown in Fig. \ref{roc} It shows the performance of five classifiers A, B, C, D and E. A classifier plotted at the point (0,0) in this graph never  gives a positive classification, hence committing no false positive errors, but not gaining any true positives as well. On the other hand, the point (1,1) represents the opposite kind of strategy. A classifier plotted at (0,1) has perfect classification. The performance of classifiers can be measured relative to these points. A classifier is better than another if it either has higher true positive rate, lower false positive rate, or both. In this case, D is the best performing model.   
\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.3]{roc-curve}
    \caption{ROC curve} \label{roc}
  \end{center}
\end{figure}

\subsection{Evaluation - Metrics}
The following table gives the metrics of the classifiers used in the previous chapter. We chose the optimum model for each type of classifier. The listed measures should be high for a good classifier, as seen from the table Logistic Regression performs the best for each metric. Since in our problem predicting the survival rate is important, the metrics with Negatives are more valuable, namely Specificity. Based on this we can say Random Forest and Decision Trees  perform in the same way. Between Naive Bayes and SVM, Naive Bayes is better since it has higher Specificity which means it predicts negatives better. Similarly between SVM and Decision Trees, latter performs better.
\begin{table}[h]
    \begin{tabular}{llllll}
    Measure              & Logistic Regression & Naive Bayes & SVM  & Decision Trees & Random Forests \\ \hline
    Precision            & 0.87                & 0.85        & 0.81 & 0.82           & 0.83           \\
    Recall / Sensitivity & 0.91                & 0.87        & 0.95 & 0.92           & 0.92           \\
    Specificity          & 0.77                & 0.74        & 0.62 & 0.67           & 0.68           \\
    Accuracy             & 0.86                & 0.82        & 0.82 & 0.83           & 0.83           \\
    \end{tabular}
    \caption{Performance Metrics of Models}
\end{table}

\subsection{Evaluation - ROC Curve}

\begin{figure}[h]
  \begin{center}
  \includegraphics[scale=0.5]{rocr}
  \caption{ROC curves for models and their Area under the Curve (auc)} \label{rocr}
  \end{center}
\end{figure}

The obtained curves in Figure\ref{rocr} corroborate the evidence from the table. The accuracy of the classifier can be determined by computing the area under it's ROC curve. The area under the curve(\emph{auc}) was calculated for all five classifiers and the results are included in \ref{rocr}. It can be observed that the classifier based on logistic regression has performed the best, while the SVM-based classifier has the worst performance for the given dataset in terms of \emph{auc} as well.
\end{document}
