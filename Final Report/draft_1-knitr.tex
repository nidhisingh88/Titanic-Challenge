

\documentclass[a4paper,10pt]{report}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[all]{hypcap}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}

\date{\today}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}






% Title Page
\begin{titlepage}
\begin{center}
% Title

\textsc{\Large SPM4450: Fundamentals of Data Analytics - Final Assignment Report }\\[6cm]

{ \bfseries \Large Performance of various Data Analytics Techniques on Kaggle's Problem Set `Titanic: Machine Learning from Disaster' \\[6cm] }

% Author and supervisor
\begin{minipage}{0.6\textwidth}
\emph{Authors:}\\
\begin{flushleft} \large
Nidhi \textsc{Singh}\\
4242246 \\
n.singh-2@student.tudelft.nl\\
MSc. Computer Science\\
\end{flushleft}

\begin{flushright} \large
K. \textsc{Chaitanya Akundi}\\
k.c.akundi@student.tudelft.nl\\
MSc. Computer Science\\
\end{flushright}

\end{minipage}

\end{center}
\end{titlepage}

\listoffigures

\chapter{Titanic Data Set}
\section{Problem Description}
For our final assignment, we have taken up a challenge from Kaggle `Predict survival on the Titanic'. The dataset includes details of people who travelled on RMS Titanic which sank in 1912 killing 1502 out of 2224 passengers.
The aim of the Kaggle challenge is to complete the analysis of what sorts of people were likely to survive. In order to do so, we will apply different predictive models to the dataset and will finally evaluate their performance against each other. Kaggle also supports Leaderboards which evaluate the submitted results, but since this evaluation is based on only 50\% of the test data, it makes sense to do performance evaluation of all the models.

\ Since we are given both training and test data set, this problem's predictive models will fall under the umbrella of Supervised Learning Algorithms. Also we have to decide whether a passenger survived or not, this makes it a classic Classification problem.

\section{Data Exploration}
Before diving deep into prediction making on test data, we will explore the dataset. We are given two sets of data, training (data containing attributes and known outcomes [survived or perished] for a subset of the passengers) and test (data containing attributes without outcomes for a subset of passengers).The given training data set has 891 observations of following 12 variables:
\begin{itemize}
  \item PassengerId - Unique generated Id for each passenger
  \item Survived - Survival(0 = No; 1 = Yes)
  \item Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)
  \item Name - Name of the person
  \item sex - Sex 
  \item Age - Age
  \item Sibsp - Number of Siblings/Spouses Aboard
  \item Parch - Number of Parents/Children Aboard
  \item Ticket - Ticket Number
  \item Fare - Passenger Fare
  \item Cabin - Cabin in the ship
  \item Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)
  
\end{itemize}


Let us start by looking at the type of these variables
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{str}\hlstd{(train_csv)}
\end{alltt}
\begin{verbatim}
## 'data.frame':	891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : Factor w/ 891 levels "Abbing, Mr. Anthony",..: 109 191 358 277 16 559 520 629 416 581 ...
##  $ Sex        : Factor w/ 2 levels "female","male": 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : Factor w/ 681 levels "110152","110413",..: 525 596 662 50 473 276 86 396 345 133 ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : Factor w/ 148 levels "","A10","A14",..: 1 83 1 57 1 1 131 1 1 1 ...
##  $ Embarked   : Factor w/ 4 levels "","C","Q","S": 4 2 4 4 4 3 4 4 4 2 ...
\end{verbatim}
\end{kframe}
\end{knitrout}
Here Factor refers to categorical data, since all the names are unique, we have 891 levels equal to number of observations.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{prop.table}\hlstd{(}\hlkwd{table}\hlstd{(train_csv}\hlopt{$}\hlstd{Survived))}
\end{alltt}
\begin{verbatim}
## 
##      0      1 
## 0.6162 0.3838
\end{verbatim}
\end{kframe}
\end{knitrout}
This shows that 61.6\% of the passengers perished and only 38.3\% survived.
Running the same code for Sex, we find 35.2\% females and 64.7\% in the training data set.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(train_csv}\hlopt{$}\hlstd{Age)}
\end{alltt}
\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    0.42   20.10   28.00   29.70   38.00   80.00     177
\end{verbatim}
\end{kframe}
\end{knitrout}
Summary results on Age shows that this variable is missing for 177 passengers and the minimum age is 0.42 or 5 months and maximum is 80, while 90\% of the passengers were below 50.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{prop.table}\hlstd{(}\hlkwd{table}\hlstd{(train_csv}\hlopt{$}\hlstd{Pclass))}
\end{alltt}
\begin{verbatim}
## 
##      1      2      3 
## 0.2424 0.2065 0.5511
\end{verbatim}
\end{kframe}
\end{knitrout}
More than 55\% passengers were travelling in third class. It will be worthwhile to see the age and sex of people in each class. 





\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassBySex}
    \caption{Passenger Class by Gender.} \label{ClassBySex}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassByAge}
    \caption{Passenger Class by Age.} \label{ClassByAge}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassByFare}
    \caption{Passenger Class by Fare.} \label{ClassByFare}
  \end{subfigure}
  \caption{Passenger class by Sex, Age and Fare}\label{Class}
\end{figure}
We see in Figure\ref{Class} that third class has mostly males, since third class cabins were at the bottom of the ship this might be one of the reasons that most of the males could not survive. Also passengers in third class were younger with median below 25.With just one outlier above \$500 for first class ticket fare, fare is below \$100.





\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Sibsp}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Parch}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Embark}
  \end{subfigure}
  \caption{Frequency of Sibsp, Parch and Embarkment}\label{Var}
\end{figure}
We now look at other variables to see if they can have some influence on predictions. From Figure \ref{Var} we can see that  most passengers travelled alone and started their journey from Southampton.

\ Other varibales `Ticket' and `Cabin' do not tell much as they have unique values, and are un-related to other variables.
\subsection{Survived variable with other variables}
Till now we looked at the variables and their values and frequencies and tried to get an initial understanding of the data.
Since we have to predict the `Survived' variable for the test set, in this section we will look at the relation between `Survived' variable and other variables.

As we can see from Figure \ref{Surv} age and fare doesnt seem to give much information about the survived variable, moreover most of the passengers were from Southampton so Place of Embarkment doesn't seem to play much role too.

But from Figure \ref{SurvCSE} we can find some interesting facts, people in 1st class outnumbered the people from 3rd class in survival rate.So there was a clear preference for elite poeple. From the second plot in Figure\ref{SurvCSE} we can see another preference was for females, we would like to believe that there was preference for children but this is not yet evident from our data. The last plot in Figure \ref{SurvCSE} shows that surely there was a clear bias for females in 1st and 2nd class compared to males. This is a clear indicator that `Sex' variable is hihgly important for our analysis with maybe `Pclass' coming next.



\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvAge}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvFare}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvEmb}
  \end{subfigure}
  \caption{Passengers Survived by Age, Fare and Place of Embarkment}\label{Surv}
\end{figure}






\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvClass}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvSex}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvSC}
  \end{subfigure}
  \caption{Passengers survived by Class and Gender}\label{SurvCSE}
\end{figure}

\section{Feature Engineering}
There are few variables which have `NA' and missing values, before we run our prediction models we need to get rid of these
\chapter{Prediction Models}
Since the `Predict survival on the Titanic' challenge is a classification problem, we will start with Linear Classifiers and then go further with methods like Decision trees and Ensembles of classifiers. In the following sections we will explore each model in detail and will also report its evaluation on Kaggle.

\section{Logistic Regression}
Logistic Regression is a classical Classification algorithm. R's \emph{glm} function is used to fit generalized linear models, With \emph{family} variable set to "binomial", glm( ) produces a logistic regression.The output of a linear regression can be transformed to a logit functions as follows:

$$
logit{p} = \log{o} = \log{p/1-p} = \beta_{0} + \beta_{1}x_{1} +\beta_{2}x_{2} + ...+\beta_{k}x_{k}
$$

Exponentiating this we get,
$$ \exp^{logit{p}} = \exp^{\beta_{0}} \exp^{\beta_{1}x_{1}} \exp^{\beta_{2}x_{2}}... \exp^{\beta_{k}x_{k}} $$
rewriting we get,
$$ o = p/1-p = \exp^{\beta_{0}} \exp^{\beta_{1}x_{1}} \exp^{\beta_{2}x_{2}}... \exp^{\beta_{k}x_{k}} $$
Here \emph{o} represents the \emph{odds}. From this we can say that if we know that a certain fact is true of a data point, then that will produce a constant change in the odds of the outcome.

We will run our first regression model with basic features provided within the dataset and can look at the results by calling summary on this model.Summary gives the value of \emph{estimated coefficients alongwith their standard errors and p-value} of each input variable. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{logit.m1} \hlkwb{<-} \hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Pclass} \hlopt{+} \hlstd{Sex} \hlopt{+} \hlstd{Age} \hlopt{+} \hlstd{SibSp} \hlopt{+}
                  \hlstd{Parch} \hlopt{+} \hlstd{Fare} \hlopt{+} \hlstd{Embarked,}
                \hlkwc{data} \hlstd{= train.batch,}
                \hlkwc{family}\hlstd{=}\hlstr{"binomial"}\hlstd{)}
\hlkwd{summary}\hlstd{(logit.m1)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## glm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked, family = "binomial", data = train.batch)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.639  -0.617  -0.446   0.666   2.424  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  1.80e+01   6.07e+02    0.03    0.976    
## Pclass      -1.10e+00   1.61e-01   -6.81  1.0e-11 ***
## Sexmale     -2.65e+00   2.23e-01  -11.93  < 2e-16 ***
## Age         -4.47e-02   8.88e-03   -5.04  4.7e-07 ***
## SibSp       -2.55e-01   1.20e-01   -2.13    0.033 *  
## Parch       -7.07e-02   1.30e-01   -0.54    0.588    
## Fare         3.04e-05   2.83e-03    0.01    0.991    
## EmbarkedC   -1.24e+01   6.07e+02   -0.02    0.984    
## EmbarkedQ   -1.26e+01   6.07e+02   -0.02    0.983    
## EmbarkedS   -1.29e+01   6.07e+02   -0.02    0.983    
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 957.90  on 712  degrees of freedom
## Residual deviance: 646.84  on 703  degrees of freedom
## AIC: 666.8
## 
## Number of Fisher Scoring iterations: 13
\end{verbatim}
\end{kframe}
\end{knitrout}

Logistic Regression uses the deviance test to estimate the goodness of the model. Its approach is based on estimating two models, it assumes that one excludes the hypothesized effects to be null, while to be included in 
the other. For each model, a deviance statistic, equal to -2 ln L for that model, is computed, which in this case is
\emph{947.99(Null model) and 641.48(estimated model)}. The deviance can be regarded as a measure of lack of fit between 
model and data. In general, the larger the deviance, the poorer the fit to the data. In our first iteration with basic
features we can see reduction of \emph{306.51} in deviance. The difference between the deviances has a large-sample chi-square distribution with degrees of freedom equal to the difference in the number of parameters estimated. Thus the difference in deviances can be tested against the chi-square distribution for significance, as done below.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pchisq}\hlstd{(}\hlnum{947.99}\hlopt{-}\hlnum{641.48}\hlstd{,} \hlnum{9}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\end{kframe}
\end{knitrout}
Also, we can see from the summary that only \emph{Pclass, Sex, Age and SibSp(moderately)} has effect on the model with significance less than .05 level. To see how these variables affect, we can also do \emph{ANOVA} test, which tries adding the factors in the given order.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{anova}\hlstd{(logit.m1,} \hlkwc{test}\hlstd{=}\hlstr{"Chisq"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: Survived
## 
## Terms added sequentially (first to last)
## 
## 
##          Df Deviance Resid. Df Resid. Dev Pr(>Chi)    
## NULL                       712        958             
## Pclass    1     68.1       711        890  < 2e-16 ***
## Sex       1    204.9       710        685  < 2e-16 ***
## Age       1     23.2       709        662  1.5e-06 ***
## SibSp     1      7.9       708        654   0.0049 ** 
## Parch     1      0.4       707        653   0.5309    
## Fare      1      0.2       706        653   0.6218    
## Embarked  3      6.2       703        647   0.1007    
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}
From the ANOVA results too, we can see that the deviance is highly reduced by \emph{Pclass, Sex} and weakly reduced by \emph{Age and SibSp} and with more or less no affect on deviance by other variables(Parch, Fare and Embarked).
To improve our model further we can make use of \emph{engineered variables} from previous chapter, namely \emph{FamilySize and FamilyID2}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{logit.m2} \hlkwb{<-} \hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Pclass} \hlopt{+} \hlstd{Sex} \hlopt{+} \hlstd{Age} \hlopt{+} \hlstd{SibSp} \hlopt{+}
                  \hlstd{Parch} \hlopt{+} \hlstd{Fare} \hlopt{+} \hlstd{Embarked} \hlopt{+}
                  \hlstd{FamilySize} \hlopt{+} \hlstd{FamilyID2,}
                \hlkwc{data} \hlstd{= train.batch,}
                \hlkwc{family}\hlstd{=}\hlstr{"binomial"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pchisq}\hlstd{(}\hlnum{365.39}\hlstd{,} \hlnum{30}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\begin{alltt}
\hlkwd{anova}\hlstd{(logit.m2,} \hlkwc{test}\hlstd{=}\hlstr{"Chisq"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: Survived
## 
## Terms added sequentially (first to last)
## 
## 
##          Df Deviance Resid. Df Resid. Dev Pr(>Chi)    
## NULL                       712        954             
## Pclass    1     73.4       711        881  < 2e-16 ***
## Sex       1    205.2       710        676  < 2e-16 ***
## Age       1     17.1       709        659  3.6e-05 ***
## SibSp     1      9.8       708        649   0.0017 ** 
## Parch     1      0.1       707        649   0.7885    
## Fare      1      4.3       706        645   0.0390 *  
## Embarked  3      3.8       703        641   0.2848    
## Title    10     55.8       693        585  2.2e-08 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}

The new model results in \emph{592.51} deviance, reducing \emph{365.39} points over null deviance of 957.90. The ANOVA test results show that the engineered variable \emph{FamilyID2} has a significant on deviance. Thus, we will be including it in our further modelling.

\section{Decision Trees}
Decision tree algorithms work by repeatedly splitting the dataset into subsets based on a particular attribute value. This process is recursively carried out until further splitting does not add any value to the predictions. This is a greedy algorithm, which means that decisions with the highest immediate value are given preference.

We applied recursive partitioning on the Titanic dataset using the Decision Tree algorithm from R's \emph{rpart} package . For this, the dependent variables used were Pclass, Sex, Age, Fare, Embarked, Title and FamilyID2. The training dataset containing 891 records was further partitioned with 80\% of it taken as the training set, and the remaining 20\% as the test set, to evaluate the performance of the model. So there were 418 records in the training partition and 178 records in the test partition. The algorithm is invoked as follows.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dt.1} \hlkwb{<-} \hlkwd{rpart}\hlstd{(Survived} \hlopt{~} \hlstd{Pclass} \hlopt{+} \hlstd{Sex} \hlopt{+} \hlstd{Age} \hlopt{+} \hlstd{Fare} \hlopt{+} \hlstd{Embarked} \hlopt{+}
    \hlstd{Title} \hlopt{+} \hlstd{FamilyID2,} \hlkwc{data} \hlstd{= train.batch,} \hlkwc{method} \hlstd{=} \hlstr{"class"}\hlstd{,}
    \hlkwc{control} \hlstd{=} \hlkwd{rpart.control}\hlstd{(}\hlkwc{minsplit} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{cp} \hlstd{=} \hlnum{0.001}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 115           & 17 \\
    1          & 7             & 39 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Initial Decision Tree}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.8652          \\
    Accuracy with unseen data(Kaggle)               & 0.78469 \\ 
    95\% CI                & (0.8061, 0.9117) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  2.329e-08      \\
    Kappa                  &  0.6715         \\
    Mcnemar's Test P-Value & 0.06619          \\
    Sensitivity            &  0.9426         \\
    Specificity            & 0.6964          \\ \hline
    \end{tabular}
    \caption{Evaluation - Initial Decision Tree}
\end{table}

\subsection{}
In the second run, we added the engineered features Title and FamilyID2 to the estimation, while removing Age, SibSp and Parch which are already composed in Title and FamilyID2.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dt.2} \hlkwb{<-} \hlkwd{rpart}\hlstd{(Survived} \hlopt{~} \hlstd{Pclass} \hlopt{+} \hlstd{Fare} \hlopt{+} \hlstd{Embarked} \hlopt{+} \hlstd{Title} \hlopt{+} \hlstd{FamilyID2,}
    \hlkwc{data} \hlstd{= train.batch,} \hlkwc{method} \hlstd{=} \hlstr{"class"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
This resulted in marginal gains in accuracy and sensitivity of the model, thus giving better results on unseen data.

\begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 117           & 18 \\
    1          & 5             & 38 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Engineered Features}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.8708          \\
    Accuracy with unseen data(Kaggle)               & 0.79426 \\ 
    95\% CI                & (0.8124, 0.9163) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  7.677e-09      \\
    Kappa                  &  0.6803         \\
    Mcnemar's Test P-Value & 0.01234          \\
    Sensitivity            &  0.9590         \\
    Specificity            & 0.6786          \\ \hline
    \end{tabular}
    \caption{Evaluation - Engineered Features}
\end{table}
\subsection{}
In the third run, we added control parameters which specified \emph{minsplit} as 2 and \emph{cp} as 0. This allowed unbounded growth for the tree, and resulted in a complex structure with large a number of branches.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{rpart}\hlstd{(Survived} \hlopt{~} \hlstd{Pclass} \hlopt{+} \hlstd{Age} \hlopt{+} \hlstd{Fare} \hlopt{+} \hlstd{Embarked} \hlopt{+} \hlstd{Title} \hlopt{+}
    \hlstd{FamilyID2,} \hlkwc{data} \hlstd{= train.batch,} \hlkwc{method} \hlstd{=} \hlstr{"class"}\hlstd{,} \hlkwc{control} \hlstd{=} \hlkwd{rpart.control}\hlstd{(}\hlkwc{minsplit} \hlstd{=} \hlnum{2}\hlstd{,}
    \hlkwc{cp} \hlstd{=} \hlnum{0}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
The results were very accurate on the training data. However, the model performed poorly with the test data. From this, we could conclude that the model is overfitting to a high degree. 
 \begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 122           & 2 \\
    1          & 0             & 54 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Unbounded Growth Tree}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.9888          \\
    Accuracy with unseen data(Kaggle)               & 0.74163 \\ 
    95\% CI                & (0.96, 0.9986) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  \textless2e-16      \\
    Kappa                  &  0.9737         \\
    Mcnemar's Test P-Value & 0.4795          \\
    Sensitivity            &  1.0000         \\
    Specificity            & 0.9643          \\ \hline
    \end{tabular}
    \caption{Evaluation - Unbounded Growth Tree}
\end{table}


\section{Support Vector Machines}

Support Vector Machine (SVM) algorithms perform classification by representing the given data as points in space, with a clear plane of separation between the different classes to which the points belong. SVMs use the 'kernel trick' to work with high-dimensional data without ever having to map the points in such spaces. 
In R, the \texttt{e1071} and \texttt{kernlab} packages offer methods for creating SVM models. We used the \texttt{kernlab} package to generate our SVM models.

\subsection{}
Initially, the algorithm was run in linear classification mode, using the features that were already present in the given dataset. The results are as follows.
\begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 119           & 17 \\
    1          & 3             & 39 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Linear SVM}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.8876          \\
    Accuracy with unseen data(Kaggle)               & 0.62679 \\ 
    95\% CI                & (0.8318, 0.93) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  2.044e-10      \\
    Kappa                  &  0.7206         \\
    Mcnemar's Test P-Value & 0.00365          \\
    Sensitivity            &  0.9754         \\
    Specificity            & 0.6964          \\ \hline
    \end{tabular}
    \caption{Evaluation - Linear SVM}
\end{table}

\end{document}
