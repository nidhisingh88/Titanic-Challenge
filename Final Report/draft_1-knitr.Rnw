

\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[all]{hypcap}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{caption}
\usepackage{subcaption}

\date{\today}

\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(stargazer)
library(vcd)
library(ggplot2)
library(rpart)

opts_chunk$set(fig.path='/home/nidhi/Courses/TUDelft-Data Analytics/Kaggle:Titanic Challenge/Final Report/figure/DA', fig.align='center',dev='pdf', dev.args=list(family='serif'), fig.pos='!ht',
concordance=TRUE,fig.width=3.5,fig.height=3.5)
options(width=60)
@


<<mychunk, cache=TRUE, eval=TRUE,include=FALSE>>=
load(file='/home/nidhi/Courses/TUDelft-Data Analytics/Kaggle:Titanic Challenge/mydata.RData')
@

% Title Page
\begin{titlepage}
\begin{center}
% Title

\textsc{\Large SPM4450: Fundamentals of Data Analytics - Final Assignment Report }\\[6cm]

{ \bfseries \Large Performance of various Data Analytics Techniques on Kaggle's Problem Set `Titanic: Machine Learning from Disaster' \\[6cm] }

% Author and supervisor
\begin{minipage}{0.6\textwidth}
\emph{Authors:}\\
\begin{flushleft} \large
Nidhi \textsc{Singh}\\
4242246 \\
n.singh-2@student.tudelft.nl\\
MSc. Computer Science\\
\end{flushleft}

\begin{flushright} \large
K. \textsc{Chaitanya Akundi}\\
k.c.akundi@student.tudelft.nl\\
MSc. Computer Science\\
\end{flushright}

\end{minipage}

\end{center}
\end{titlepage}

\listoffigures

\chapter{Titanic Data Set}
\section{Problem Description}
For our final assignment, we have taken up a challenge from Kaggle `Predict survival on the Titanic'. The dataset includes details of people who travelled on RMS Titanic which sank in 1912 killing 1502 out of 2224 passengers.
The aim of the Kaggle challenge is to complete the analysis of what sorts of people were likely to survive. In order to do so, we will apply different predictive models to the dataset and will finally evaluate their performance against each other. Kaggle also supports Leaderboards which evaluate the submitted results, but since this evaluation is based on only 50\% of the test data, it makes sense to do performance evaluation of all the models.

\ Since we are given both training and test data set, this problem's predictive models will fall under the umbrella of Supervised Learning Algorithms. Also we have to decide whether a passenger survived or not, this makes it a classic Classification problem.

\section{Data Exploration}
Before diving deep into prediction making on test data, we will explore the dataset. We are given two sets of data, training (data containing attributes and known outcomes [survived or perished] for a subset of the passengers) and test (data containing attributes without outcomes for a subset of passengers).The given training data set has 891 observations of following 12 variables:
\begin{itemize}
  \item PassengerId - Unique generated Id for each passenger
  \item Survived - Survival(0 = No; 1 = Yes)
  \item Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)
  \item Name - Name of the person
  \item sex - Sex 
  \item Age - Age
  \item Sibsp - Number of Siblings/Spouses Aboard
  \item Parch - Number of Parents/Children Aboard
  \item Ticket - Ticket Number
  \item Fare - Passenger Fare
  \item Cabin - Cabin in the ship
  \item Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)
  
\end{itemize}

<<echo=FALSE>>=

train_csv <- read.csv('/home/nidhi/Courses//TUDelft-Data Analytics/Kaggle:Titanic problem/train.csv')
@
Let us start by looking at the type of these variables
<<>>=
str(train_csv)
@
Here Factor refers to categorical data, since all the names are unique, we have 891 levels equal to number of observations.
<<>>=
prop.table(table(train_csv$Survived))
@
This shows that 61.6\% of the passengers perished and only 38.3\% survived.
Running the same code for Sex, we find 35.2\% females and 64.7\% in the training data set.

<<>>=
summary(train_csv$Age)
@
Summary results on Age shows that this variable is missing for 177 passengers and the minimum age is 0.42 or 5 months and maximum is 80, while 90\% of the passengers were below 50.
<<>>=
prop.table(table(train_csv$Pclass))
@
More than 55\% passengers were travelling in third class. It will be worthwhile to see the age and sex of people in each class. 

<<include=FALSE,label=ClassBySex,echo=FALSE>>=
mosaicplot(train_csv$Pclass ~ train_csv$Sex, 
           main='Passenger Class by Gender',
           shade=FALSE, 
           color=TRUE, xlab="Pclass", ylab="Sex of passenger")
@

<<include=FALSE,label=ClassByAge,echo=FALSE>>=
bpa <- ggplot(train_csv, aes(factor(Pclass), Age))
bpa + geom_boxplot()
@
<<include=FALSE,label=ClassByFare,echo=FALSE>>=
bpf <- ggplot(train_csv, aes(factor(Pclass), Fare))
bpf + geom_boxplot()+ylim(c(0,300))
@
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassBySex}
    \caption{Passenger Class by Gender.} \label{ClassBySex}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassByAge}
    \caption{Passenger Class by Age.} \label{ClassByAge}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-ClassByFare}
    \caption{Passenger Class by Fare.} \label{ClassByFare}
  \end{subfigure}
  \caption{Passenger class by Sex, Age and Fare}\label{Class}
\end{figure}
We see in Figure\ref{Class} that third class has mostly males, since third class cabins were at the bottom of the ship this might be one of the reasons that most of the males could not survive. Also passengers in third class were younger with median below 25.With just one outlier above \$500 for first class ticket fare, fare is below \$100.

<<include=FALSE,label=Sibsp,echo=FALSE>>=
qplot(train_csv$SibSp,main='Travelling with Siblings/Spouse',xlab='No. of Siblings/Spouse')
@
<<include=FALSE,label=Parch,echo=FALSE>>=
qplot(train_csv$Parch,main='Travelling with Parents/Children',xlab='No. of Parents/Children')
@
<<include=FALSE,label=Embark,echo=FALSE>>=
qplot(train_csv$Embarked,main='Travelers Place of Embarkment',xlab='Place of Embarkment')
@

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Sibsp}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Parch}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-Embark}
  \end{subfigure}
  \caption{Frequency of Sibsp, Parch and Embarkment}\label{Var}
\end{figure}
We now look at other variables to see if they can have some influence on predictions. From Figure \ref{Var} we can see that  most passengers travelled alone and started their journey from Southampton.

\ Other varibales `Ticket' and `Cabin' do not tell much as they have unique values, and are un-related to other variables.
\subsection{Survived variable with other variables}
Till now we looked at the variables and their values and frequencies and tried to get an initial understanding of the data.
Since we have to predict the `Survived' variable for the test set, in this section we will look at the relation between `Survived' variable and other variables.

As we can see from Figure \ref{Surv} age and fare doesnt seem to give much information about the survived variable, moreover most of the passengers were from Southampton so Place of Embarkment doesn't seem to play much role too.

But from Figure \ref{SurvCSE} we can find some interesting facts, people in 1st class outnumbered the people from 3rd class in survival rate.So there was a clear preference for elite poeple. From the second plot in Figure\ref{SurvCSE} we can see another preference was for females, we would like to believe that there was preference for children but this is not yet evident from our data. The last plot in Figure \ref{SurvCSE} shows that surely there was a clear bias for females in 1st and 2nd class compared to males. This is a clear indicator that `Sex' variable is hihgly important for our analysis with maybe `Pclass' coming next.
<<include=FALSE,label=SurvAge,echo=FALSE>>=
bsa <- ggplot(train_csv, aes(factor(Survived), Age))
bsa + geom_boxplot() + geom_jitter()
@
<<include=FALSE,label=SurvFare,echo=FALSE>>=
bsf <- ggplot(train_csv, aes(factor(Survived), Fare))
bsf + geom_boxplot()+ylim(c(0,300))
@
<<include=FALSE,label=SurvEmb,echo=FALSE>>=
doubledecker(Survived ~ Embarked, data=train_csv)
@
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvAge}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvFare}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvEmb}
  \end{subfigure}
  \caption{Passengers Survived by Age, Fare and Place of Embarkment}\label{Surv}
\end{figure}
<<include=FALSE,label=SurvClass,echo=FALSE>>=
doubledecker(Survived ~ Pclass, data=train_csv)
@

<<include=FALSE,label=SurvSex,echo=FALSE>>=
doubledecker(Survived ~ Sex, data=train_csv)
@
<<include=FALSE,label=SurvSC,echo=FALSE>>=
doubledecker(Survived ~ Sex + Pclass, data=train_csv)
@


\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvClass}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvSex}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{DA-SurvSC}
  \end{subfigure}
  \caption{Passengers survived by Class and Gender}\label{SurvCSE}
\end{figure}

\section{Feature Engineering}
There are few variables which have `NA' and missing values, before we run our prediction models we need to get rid of these
\chapter{Prediction Models}
Since the `Predict survival on the Titanic' challenge is a classification problem, we will start with Linear Classifiers and then go further with methods like Decision trees and Ensembles of classifiers. In the following sections we will explore each model in detail and will also report its evaluation on Kaggle.

\section{Logistic Regression}
Logistic Regression is a classical Classification algorithm. R's \emph{glm} function is used to fit generalized linear models, With \emph{family} variable set to "binomial", glm( ) produces a logistic regression.

We will run our first regression model with basic features provided within the dataset and can look at the results by calling summary on this model.
<<eval=TRUE>>=
logit.m1 <- glm(Survived ~ Pclass + Sex + Age + SibSp + 
                  Parch + Fare + Embarked, 
                data = train.batch, 
                family="binomial")
summary(logit.m1)
@


\section{Decision Trees}
Decision tree algorithms work by repeatedly splitting the dataset into subsets based on a particular attribute value. This process is recursively carried out until further splitting does not add any value to the predictions. This is a greedy algorithm, which means that decisions with the highest immediate value are given preference.

We applied recursive partitioning on the Titanic dataset using the Decision Tree algorithm from R's \emph{rpart} package . For this, the dependent variables used were Pclass, Sex, Age, Fare, Embarked, Title and FamilyID2. The training dataset containing 891 records was further partitioned with 80\% of it taken as the training set, and the remaining 20\% as the test set, to evaluate the performance of the model. So there were 418 records in the training partition and 178 records in the test partition. The algorithm is invoked as follows.
<<eval=T,echo=T,tidy=TRUE>>=
dt.1 <- rpart(Survived ~ Pclass + Sex + Age + Fare + Embarked + Title + FamilyID2,
              data = train.batch, method = "class", control = rpart.control(minsplit = 2, cp = 0.001))
@


\begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 115           & 17 \\
    1          & 7             & 39 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Initial Decision Tree}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.8652          \\
    Accuracy with unseen data(Kaggle)               & 0.78469 \\ 
    95\% CI                & (0.8061, 0.9117) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  2.329e-08      \\
    Kappa                  &  0.6715         \\
    Mcnemar's Test P-Value & 0.06619          \\
    Sensitivity            &  0.9426         \\
    Specificity            & 0.6964          \\ \hline
    \end{tabular}
    \caption{Evaluation - Initial Decision Tree}
\end{table}

\subsection{}
In the second run, we added the engineered features Title and FamilyID2 to the estimation, while removing Age, SibSp and Parch which are already composed in Title and FamilyID2.
<<eval=T,echo=T,tidy=TRUE>>=
dt.2 <- rpart(Survived ~ Pclass + Fare + Embarked + 
             Title + FamilyID2, data=train.batch,
             method="class")
@
This resulted in marginal gains in accuracy and sensitivity of the model, thus giving better results on unseen data.

\begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 117           & 18 \\
    1          & 5             & 38 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Engineered Features}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.8708          \\
    Accuracy with unseen data(Kaggle)               & 0.79426 \\ 
    95\% CI                & (0.8124, 0.9163) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  7.677e-09      \\
    Kappa                  &  0.6803         \\
    Mcnemar's Test P-Value & 0.01234          \\
    Sensitivity            &  0.9590         \\
    Specificity            & 0.6786          \\ \hline
    \end{tabular}
    \caption{Evaluation - Engineered Features}
\end{table}
\subsection{}
In the third run, we added control parameters which specified \emph{minsplit} as 2 and \emph{cp} as 0. This allowed unbounded growth for the tree, and resulted in a complex structure with large a number of branches.
<<eval=T,echo=T,tidy=TRUE>>=
fit <- rpart(Survived ~ Pclass  + Age  + Fare + Embarked 
             + Title  + FamilyID2, data=train.batch,
             method="class",control=rpart.control(minsplit=2,cp=0.0))
@
The results were very accurate on the training data. However, the model performed poorly with the test data. From this, we could conclude that the model is overfitting to a high degree. 
 \begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 122           & 2 \\
    1          & 0             & 54 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Unbounded Growth Tree}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.9888          \\
    Accuracy with unseen data(Kaggle)               & 0.74163 \\ 
    95\% CI                & (0.96, 0.9986) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  \textless2e-16      \\
    Kappa                  &  0.9737         \\
    Mcnemar's Test P-Value & 0.4795          \\
    Sensitivity            &  1.0000         \\
    Specificity            & 0.9643          \\ \hline
    \end{tabular}
    \caption{Evaluation - Unbounded Growth Tree}
\end{table}


\section{Support Vector Machines}

Support Vector Machine (SVM) algorithms perform classification by representing the given data as points in space, with a clear plane of separation between the different classes to which the points belong. SVMs use the 'kernel trick' to work with high-dimensional data without ever having to map the points in such spaces. 
In R, the \texttt{e1071} and \texttt{kernlab} packages offer methods for creating SVM models. We used the \texttt{kernlab} package to generate our SVM models.

\subsection{}
Initially, the algorithm was run in linear classification mode, using the features that were already present in the given dataset. The results are as follows.
\begin{table}
    \begin{tabular}{|l|l|l|}
    \hline
    ~          & Actual Value  & ~  \\ \hline
    Prediction & 0             & 1  \\ \hline
    0          & 119           & 17 \\
    1          & 3             & 39 \\ \hline
    \end{tabular}
    \caption{Confusion Matrix - Linear SVM}
\end{table}

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Statistic              & Value           \\ \hline
    Accuracy               & 0.8876          \\
    Accuracy with unseen data(Kaggle)               & 0.62679 \\ 
    95\% CI                & (0.8318, 0.93) \\
    No Information Rate    & 0.6854          \\
    P-Value [Acc \textgreater NIR]    &  2.044e-10      \\
    Kappa                  &  0.7206         \\
    Mcnemar's Test P-Value & 0.00365          \\
    Sensitivity            &  0.9754         \\
    Specificity            & 0.6964          \\ \hline
    \end{tabular}
    \caption{Evaluation - Linear SVM}
\end{table}

\end{document}
